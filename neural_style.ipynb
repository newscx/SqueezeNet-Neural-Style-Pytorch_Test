{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# some functions to deal with image\n",
    "def imload(image_name,**kwargs):\n",
    "    # a function to load image and transfer to Pytorch Variable.\n",
    "    image = Image.open(image_name)\n",
    "    if 'resize' in kwargs:\n",
    "        resize = transforms.Scale(kwargs['resize'])\n",
    "        image = resize(image)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),#Converts (H x W x C) of[0, 255] to (C x H x W) of range [0.0, 1.0]. \n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
    "    image = Variable(transform(image),volatile=True)\n",
    "    image = image.unsqueeze(0) \n",
    "    return image\n",
    "\n",
    "def imshow(img):\n",
    "    # convert torch tensor to PIL image and then show image inline.\n",
    "    img=toImage(img[0].data*0.5+0.5) # denormalize tensor before convert\n",
    "    plt.imshow(img,aspect=None)\n",
    "    plt.axis('off')\n",
    "    plt.gcf().set_size_inches(8, 8)\n",
    "    plt.show()\n",
    "\n",
    "def imsave(img,path):\n",
    "    # convert torch tensor to PIL image and then save to path\n",
    "    img=toImage(img[0].data*0.5+0.5) # denormalize tensor before convert\n",
    "    img.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtracter(nn.Module):\n",
    "    # a nn.Module class to extract a intermediate activation of a Torch module\n",
    "    def __init__(self,submodule):\n",
    "        super().__init__()\n",
    "        self.submodule = submodule\n",
    "    def forward(self,image,layers):\n",
    "        features = []\n",
    "        for i in range(layers[-1]+1):\n",
    "            image = self.submodule[i](image)\n",
    "            if i in layers :\n",
    "                features.append(image)       \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    # a nn.Module class to build gram matrix as style feature\n",
    "    def forward(self,style_features):\n",
    "        gram_features=[]\n",
    "        for feature in style_features:\n",
    "            n,f,h,w = feature.size()\n",
    "            feature = feature.resize(n*f,h*w)\n",
    "            gram_features.append((feature@feature.t()).div_(2*n*f*h*w))\n",
    "        return gram_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Stylize(nn.Module): \n",
    "    # net \n",
    "    def forward(self,x):\n",
    "        s_feats = feature(x,STYLE_LAYER)\n",
    "        s_feats = gram(s_feats)\n",
    "        c_feats = feature(x,CONTENT_LAYER)\n",
    "        return s_feats,c_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def totalloss(style_refs,content_refs,style_features,content_features,style_weight,content_weight):\n",
    "    # compute total loss \n",
    "    style_loss = [l2loss(style_features[i],style_refs[i]) for i in range(len(style_features))] \n",
    "    #a small trick to balance the influnce of diffirent style layer\n",
    "    mean_loss = sum(style_loss).data[0]/len(style_features)\n",
    "    \n",
    "    style_loss = sum([(mean_loss/l.data[0])*l*STYLE_LAYER_WEIGHTS[i] \n",
    "                    for i,l in enumerate(style_loss)])/len(style_features) \n",
    "    \n",
    "    content_loss = sum([l2loss(content_features[i],content_refs[i]) \n",
    "                    for i in range(len(content_refs))])/len(content_refs)\n",
    "    total_loss = style_weight*style_loss+content_weight*content_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the definition of the the correction factor is here , where l_i is the style loss for layer i:\n",
    "\\begin{equation*}\n",
    "\\ l_{mean} = \\frac{\\sum_{k=1}^n l_i}{n} \\\\\n",
    "\\ f_{i} = \\frac{l_{mean}}{l_i} \\\\\n",
    "\\ loss = \\sum_{k=1}^n l_i  f_i\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def reference(style_img,content_img):\n",
    "    # a function to compute style and content refenrences as used for loss\n",
    "    style_refs = feature(style_img,STYLE_LAYER)\n",
    "    style_refs = gram(style_refs)\n",
    "    style_refs = [Variable(i.data) for i in style_refs]\n",
    "    content_refs = feature(content_img,CONTENT_LAYER)\n",
    "    content_refs = [Variable(i.data) for i in content_refs]\n",
    "    return style_refs,content_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# init paramters\n",
    "learning_rate = 1e-1\n",
    "style_weight = 1#\n",
    "content_weight = 1e-3\n",
    "num_iters = 500\n",
    "report_intvl = 20\n",
    "\n",
    "# load  pretrained squeezeNet and use the first sequential\n",
    "model = models.squeezenet1_1(pretrained=True)\n",
    "submodel = next(model.children())\n",
    "\n",
    "# load image\n",
    "style_img = imload(\"img_data/style/starry_night.jpg\",resize = 224)\n",
    "content_img = imload(\"img_data/img/hohnsensee2.jpg\")\n",
    "\n",
    "# set net parameter\n",
    "STYLE_LAYER =[1,2,3,4,6,7,9]# could add more,maximal to 12\n",
    "STYLE_LAYER_WEIGHTS = [21,21,1,1,1,7,7]# this should be small length as STYLE_LAYER\n",
    "CONTENT_LAYER = [1,2,3]\n",
    "\n",
    "# build net component\n",
    "gram = GramMatrix()\n",
    "feature = FeatureExtracter(submodel)\n",
    "l2loss = nn.MSELoss(size_average=False)\n",
    "stylize = Stylize()\n",
    "toImage = transforms.ToPILImage()\n",
    "\n",
    "# init a trainable img\n",
    "train_img = Variable(torch.randn(content_img.size()),requires_grad = True)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam([train_img], lr = learning_rate)\n",
    "\n",
    "# tracers\n",
    "loss_history = [] \n",
    "min_loss = float(\"inf\")\n",
    "best_img = 0\n",
    "\n",
    "# forward\n",
    "style_refs,content_refs = reference(style_img,content_img)\n",
    "\n",
    "Start = datetime.now()\n",
    "for i in range(num_iters):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_img.data.clamp_(-1,1)  # useful at first several step\n",
    "\n",
    "    style_features,content_features = stylize(train_img)\n",
    "\n",
    "    loss = totalloss(style_refs,content_refs,style_features,content_features,style_weight,content_weight)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    loss_history.append(loss.data[0])\n",
    "\n",
    "    # save best result before update train_img\n",
    "    \n",
    "    if min_loss > loss_history[-1]:\n",
    "            min_Loss = loss_history[-1]\n",
    "            best_img = train_img\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    # report loss and image  \n",
    "    if i % report_intvl == 0:\n",
    "        print(\"step: %d loss: %f,time per example:%s s\" \n",
    "              %(i,loss_history[-1],(datetime.now()-Start)/report_intvl))\n",
    "        Start = datetime.now()\n",
    "        imshow(train_img)\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.show()\n",
    "#print(train_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train another time with smaller learning rate\n",
    "optimizer = optim.Adam([train_img], lr = learning_rate/10)\n",
    "Start = datetime.now()\n",
    "for i in range(num_iters):\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    train_img.data.clamp_(-1,1)  # useful at first several step\n",
    "    \n",
    "    style_features,content_features = stylize(train_img)\n",
    "    \n",
    "    loss = totalloss(style_refs,content_refs,style_features,content_features,style_weight,content_weight)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    loss_history.append(loss.data[0])\n",
    "    \n",
    "    # save best result before update train_img\n",
    "    if min_loss > loss_history[-1]:\n",
    "        min_Loss = loss_history[-1]\n",
    "        best_img = train_img\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    # report loss and image  \n",
    "    if i % report_intvl == 0:\n",
    "        print(\"step: %d loss: %f,time per example:%s s\" \n",
    "              %(i,loss_history[-1],(datetime.now()-Start)/report_intvl))\n",
    "        Start = datetime.now()\n",
    "        imshow(train_img)\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "imshow(Best_Img)\n",
    "imsave(Best_Img,\"img_data/output/s_ms03.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "content_img = imload(\"img_data/img/shanghai2.jpg\",resize=600)\n",
    "toImage(content_img[0].data*0.5+0.5).size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
